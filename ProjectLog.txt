Project Log

1/17/17
20120923 304 full region w/ Lowess Filter applied

for l in range(0,SPECTRA.shape[0]):
    for m in range(0,SPECTRA.shape[1]):                                            
        f = freqs  # frequencies
        s = spectra_array[l][m]  # fourier power
        filtered = lowess(s, f, is_sorted=True, frac=0.025, it=0) 
        s = filtered[:,1]

results looked pretty identical - there might be a shift in the slope histogram, but overall all the stuctures are the same in both


1/18/17
wanted to look into CUDA again, to see if it had a curve-fitting function/library.
I didn't see any

I know it has FFT libraries, but I still can't fully understand what is happening in the code.

I looked at accelerate, it has a FFT function, I wanted to see how much faster it was than what I was using, if at all.

The accelerate FFT is using numpys .fft, I had been using scipys.  I did a quick test to see the difference between those two, and for 10x500 pixels, at 6 segments, scipy took 12 seconds and numpy took 18.  I didn't check to see if the results were identical.

Think I should probably change number of points to n-power of 2.

numpy.rfft - 14 seconds

np.fft.rfft gave exact same results as np.fft.fft

tried accelerate.mkl.fftpack.fft - 7 seconds (gave warning about trial expires in 30 days?)
-read that trial is an error - can remove if bothers
accelerate.mkl.fftpack.fft identical results to np.fft.rfft

accelerate.mkl.fftpack.rfft - 6 seconds 

great page:
http://numba.pydata.org/numba-doc/0.30.1/index.html


1/20
tried using seed parameters from neighboring pixel again, this time with dogbox method.
- thought it might get rid of banding issue -- not sure why.
- instead, banding just as bad as ever - guess it is sticking to those values even more
- its too bad, because the region I was running was 300x400 and the non-initial-guess took around 1600 seconds, the initial-guess took 1100

- 6 level param loop w/10 bins 
- 1st try - didn't work, chi didnt drop below 50 
- bounds on params are too wide to get pl_A / pl_C
- entered those two, and varied the other 4, and got low chi of 6
- curve_fit gets 3.5

- tried taking parameters from previous pixel, and using 10 bins, 5 above and below, to fit to model.
- seemed to get decent results, chi still not exactly as good as curve_fit, but close:
(3.83 vs 3.71) when using /100*[-5,5]
(3.73 vs 3.71) when using /10*[-5,5]

- Numba strives to support as much of the Python language as possible, but some language features are not available inside Numba-compiled functions. The following Python language features are not currently supported: Function definition

- full region finally finished on lab computer.  ended up taking 370,000 seconds = 103 hours.  I was pretty anxious to see the results, and they turned out glorious!
- the only unfortunate thing is that the region is too big to be interacted with, either by the plotly contour scripts, or 3D scatterplots.  


1/21
- tried changing curve_fit [ftol: changed from default (1e-8) to 1e-12] -- didnt change anything

- if I derotate the full region for each - would that be all I needed - could always do sub-regions from that?

- how many more results do we need?

- what data-types can I reduce?  float 64 to 32?  would make calculations quicker?  

tried float32 vs float64 on 20130530 1600 rebin4 
- float32 chi^2 way higher (36 vs 15) - although map still looks decent
- something went wrong with gaussian width - same value for all 0.425 (center of bounds?)
- same with gaussian location
- gaussian amplitude looks good (still not as detailed maybe as float64, but good)
- gaussian amp has lower amplitudes (0.011 vs 0.016 for max values)
- power-law tail horrible - none even negative like most of float64 were
- power-law index - stuck at 2.15 for all
- power-law slope coefficients - still can see structure (but opposite regions higher values - ?!?)
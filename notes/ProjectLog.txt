Project Log

1/17/17
20120923 304 full region w/ Lowess Filter applied

for l in range(0,SPECTRA.shape[0]):
    for m in range(0,SPECTRA.shape[1]):                                            
        f = freqs  # frequencies
        s = spectra_array[l][m]  # fourier power
        filtered = lowess(s, f, is_sorted=True, frac=0.025, it=0) 
        s = filtered[:,1]

results looked pretty identical - there might be a shift in the slope histogram, but overall all the stuctures are the same in both


1/18/17
wanted to look into CUDA again, to see if it had a curve-fitting function/library.
I didn't see any

I know it has FFT libraries, but I still can't fully understand what is happening in the code.

I looked at accelerate, it has a FFT function, I wanted to see how much faster it was than what I was using, if at all.

The accelerate FFT is using numpys .fft, I had been using scipys.  I did a quick test to see the difference between those two, and for 10x500 pixels, at 6 segments, scipy took 12 seconds and numpy took 18.  I didn't check to see if the results were identical.

Think I should probably change number of points to n-power of 2.

numpy.rfft - 14 seconds

np.fft.rfft gave exact same results as np.fft.fft

tried accelerate.mkl.fftpack.fft - 7 seconds (gave warning about trial expires in 30 days?)
-read that trial is an error - can remove if bothers
accelerate.mkl.fftpack.fft identical results to np.fft.rfft

accelerate.mkl.fftpack.rfft - 6 seconds 

great page:
http://numba.pydata.org/numba-doc/0.30.1/index.html


1/20
tried using seed parameters from neighboring pixel again, this time with dogbox method.
- thought it might get rid of banding issue -- not sure why.
- instead, banding just as bad as ever - guess it is sticking to those values even more
- its too bad, because the region I was running was 300x400 and the non-initial-guess took around 1600 seconds, the initial-guess took 1100

- 6 level param loop w/10 bins 
- 1st try - didn't work, chi didnt drop below 50 
- bounds on params are too wide to get pl_A / pl_C
- entered those two, and varied the other 4, and got low chi of 6
- curve_fit gets 3.5

- tried taking parameters from previous pixel, and using 10 bins, 5 above and below, to fit to model.
- seemed to get decent results, chi still not exactly as good as curve_fit, but close:
(3.83 vs 3.71) when using /100*[-5,5]
(3.73 vs 3.71) when using /10*[-5,5]

- Numba strives to support as much of the Python language as possible, but some language features are not available inside Numba-compiled functions. The following Python language features are not currently supported: Function definition

- full region finally finished on lab computer.  ended up taking 370,000 seconds = 103 hours.  I was pretty anxious to see the results, and they turned out glorious!
- the only unfortunate thing is that the region is too big to be interacted with, either by the plotly contour scripts, or 3D scatterplots.  


1/21
- tried changing curve_fit [ftol: changed from default (1e-8) to 1e-12] -- didnt change anything

- if I derotate the full region for each - would that be all I needed - could always do sub-regions from that?

- how many more results do we need?

- what data-types can I reduce?  float 64 to 32?  would make calculations quicker?  

tried float32 vs float64 on 20130530 1600 rebin4 
- float32 chi^2 way higher (36 vs 15) - although map still looks decent
- something went wrong with gaussian width - same value for all 0.425 (center of bounds?)
- same with gaussian location
- gaussian amplitude looks good (still not as detailed maybe as float64, but good)
- gaussian amp has lower amplitudes (0.011 vs 0.016 for max values)
- power-law tail horrible - none even negative like most of float64 were
- power-law index - stuck at 2.15 for all
- power-law slope coefficients - still can see structure (but opposite regions higher values - ?!?)

1/23
- for hdf5 of 20130530 193 full region - didn't use exact time array - should be same though

- if made param file float32, would that help the scatter plots / contour maps? (probably not I guess)

- need to get all files in same name structure (get rid of 'float' / 'dogbox'...)

1/26
- "f-test sensitive to non-normality of variances"?
- not liking the ds=0.1*s
- f-test heatmap for 20130815 193 looks great - add in if above threshold, then black/white -- or maybe use black/grey/white gradient colormap

1/29
- datacube size is obstacle right now - looking for way to keep only size necessary.  
- .fits images appear to be int16, and are being saved in the datacube as float64, because they are being divided by the exposure time, which is float64
- probably should save datacube as int16, reducing size by 4x, and also save exposure time array.
- in fft_avg function, when extracting pixel intensity values, put exposure time in that loop - shouldn't take significant time (never need huge array size)

- to free up some space, delete either datacubes or data off lab computer

- was concerned with how MPI would handle an array where the remainder of the chunk size wasn't 0, but it handled it perfectly.  (18,18,18,17)
- MPI: when one processor is done, the threads still in process are shared onto those 

- ran spec_fit using MPI -n 4, for 20120923 211 327x572 region, took 1170 seconds.  The results looked good, except possibly the power-law tail values were
  maybe off by 1/2.  Same structure, but were 0.002 instead of 0.003 on the high end.  I don't know if the last time I ran it I used the same parameters though.
- not using MPI, same region estimated time ~ 2900 seconds = ~2.5 speedup  

- should I build in a quick 1-row regular spec_fit, just to get an estimated time until completion?  Probably could just get average for a pixel and multiply by
  region size 

- changed datacube to int16 and worked as expected.  size reduced by 4x, and no speed penalty normalizing by exposure time in loop where extract pixel intensity.
- verified that datacubes were identical - originally saving as float, and after saving as int and dividing by exposure time later. 4GB -- 1GB 300x600

- using MPI built into fft_avg (before 3x3 averaging), and spec_fit - for 75x150 1600 rebin4 region:
- fft_avg: 7 seconds (vs ~ 25)
- spec_fit: 66 seconds (vs ~ 148)
- checked heatmaps - identical 
- was NOT using (int) datacube + exposure array, still need to do that

1/30
- ran 193 region through whole program, with int datacube, results were visually identical (might have been slightly off due to deleting 2 files)
- spec_fit using MPI with 4-cores took 5650 seconds.   

1/30
- ugh, solar derotation: differential rotation might be issue 
- derotation function derotates based on center latitude in mapcube
- for latitudes near equator that rotate faster, they are shifted more, latitudes near poles rotate slower, should be shifted less

- test - 12 images (12 hours) 20130530_171, [-500,500]x[-500,500] arcsecond region
- layer=0:  top eigth: total shift=85, dimensions = 205x1525
	    top quarter: total shift=89, dimensions = 413x1518
	    top half: total shift=96, dimensions = 830x1507
	    full: total shift=101, dimensions = 1664x1499

1/31
parallelization reading:
- http://stackoverflow.com/questions/5338338/performance-gain-using-mpi (memory bandwith issues?)
- https://rc.fas.harvard.edu/wp-content/uploads/2013/03/MPI_Plamen_Krastev.pdf (allocate specific memory for each MPI process)

- for 20130815_193 spec_fit, 4 cores = 886 seconds